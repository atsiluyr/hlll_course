{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNNs.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4gTnFnzrOLPb"
      },
      "source": [
        "# An Introduction to Convolutional Neural Networks\n",
        "\n",
        "\n",
        "We’ll build on a basic background knowledge of neural networks and <b>explore what CNNs are, understand how they work, and build a real one from scratch</b> (using only numpy) in Python."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VG8Uw2z-I9aa"
      },
      "source": [
        "##1. Motivation\n",
        "\n",
        "A classic use case of CNNs is to perform image classification, e.g. looking at an image of a pet and deciding whether it’s a cat or a dog. It’s a seemingly simple task - <b>why not just use a normal Neural Network?</b>\n",
        "\n",
        "### Reason 1: Images are Big\n",
        "\n",
        "Images used for Computer Vision problems nowadays are often 224x224 or larger. Imagine building a neural network to process 224x224 color images: including the 3 color channels (RGB) in the image, that comes out to 224 x 224 x 3 = <b>150,528</b> input features! A typical hidden layer in such a network might have 1024 nodes, so we’d have to train 150,528 x 1024 = <b>150+ million weights for the first layer alone</b>. Our network would be huge and nearly impossible to train.\n",
        "\n",
        "</br>\n",
        "\n",
        "It’s not like we need that many weights, either. The nice thing about images is that we know <b>pixels are most useful in the context of their neighbors</b>. Objects in images are made up of small, localized features, like the circular iris of an eye or the square corner of a piece of paper. Doesn’t it seem wasteful for every node in the first hidden layer to look at every pixel?\n",
        "\n",
        "### Reason 2: Positions can change\n",
        "\n",
        "If you trained a network to detect dogs, you’d want it to be able to a detect a dog regardless of where it appears in the image. Imagine training a network that works well on a certain dog image, but then feeding it a slightly shifted version of the same image. The dog would not activate the same neurons, so <b>the network would react completely differently!</b>\n",
        "\n",
        "</br>\n",
        "\n",
        "We’ll see soon how a CNN can help us mitigate these problems.\n",
        "\n",
        "## 2. Dataset\n",
        "\n",
        "In this notebook, we’ll tackle the “Hello, World!” of Computer Vision: the <b>MNIST</b> handwritten digit classification problem. It’s simple: given an image, classify it as a digit.\n",
        "\n",
        "</br>\n",
        "\n",
        "![Sample_MNIST_dataset](https://github.com/korobool/hlll_course/blob/master/topics/img/Sample_MNIST_dataset.png?raw=1)\n",
        "\n",
        "</br>\n",
        "\n",
        "Each image in the MNIST dataset is 28x28 and contains a centered, grayscale digit.\n",
        "\n",
        "</br>\n",
        "\n",
        "Truth be told, a normal neural network would actually work just fine for this problem. You could treat each image as a 28 x 28 = 784-dimensional vector, feed that to a 784-dim input layer, stack a few hidden layers, and finish with an output layer of 10 nodes, 1 for each digit.\n",
        "\n",
        "</br>\n",
        "\n",
        "This would only work because the MNIST dataset contains <b>small</b> images that are <b>centered</b>, so we wouldn’t run into the aforementioned issues of size or shifting. Keep in mind throughout the course of this post, however, that <b>most real-world image classification problems aren’t this easy.</b>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7lAjmuXANXAj"
      },
      "source": [
        "## 3. Convolutions\n",
        "\n",
        "What are Convolutional Neural Networks?\n",
        "\n",
        "</br>\n",
        "\n",
        "They’re basically just neural networks that use <b>Convolutional layers</b>, a.k.a. Conv layers, which are based on the mathematical operation of <b>convolution</b>. Conv layers consist of a set of <b>filters</b>, which you can think of as just 2d matrices of numbers. Here’s an example 3x3 filter:\n",
        "\n",
        "</br>\n",
        "\n",
        "![A_3x3_filter](https://github.com/korobool/hlll_course/blob/master/topics/img/A_3x3_filter.png?raw=1)\n",
        "\n",
        "</br>\n",
        "\n",
        "We can use an input image and a filter to produce an output image by <b>convolving</b> the filter with the input image. This consists of\n",
        "\n",
        "\n",
        "1.   Overlaying the filter on top of the image at some location.\n",
        "2.   Performing <b>element-wise multiplication</b> between the values in the filter and their corresponding values in the image.\n",
        "3.   Summing up all the element-wise products. This sum is the output value for the <b>destination pixel</b> in the output image.\n",
        "4.   Repeating for all locations.\n",
        "\n",
        "<i>Side Note: We (along with many CNN implementations) are technically actually using cross-correlation instead of convolution here, but they do almost the same thing. I won’t go into the difference in this post because it’s not that important, but feel free to look this up if you’re curious.</i>\n",
        "\n",
        "</br>\n",
        "\n",
        "That 4-step description was a little abstract, so let’s do an example. Consider this tiny 4x4 grayscale image and this 3x3 filter:\n",
        "\n",
        "</br>\n",
        "\n",
        "![A4x4image_and_3x3filter](https://github.com/korobool/hlll_course/blob/master/topics/img/A4x4image_and_3x3filter.png?raw=1)\n",
        "\n",
        "</br>\n",
        "\n",
        "The numbers in the image represent pixel intensities, where 0 is black and 255 is white. We’ll convolve the input image and the filter to produce a 2x2 output image:\n",
        "\n",
        "</br>\n",
        "\n",
        "![A2x2output_image](https://github.com/korobool/hlll_course/blob/master/topics/img/A2x2output_image.png?raw=1)\n",
        "\n",
        "</br>\n",
        "\n",
        "To start, lets overlay our filter in the top left corner of the image:\n",
        "\n",
        "</br>\n",
        "\n",
        "![Step1](https://github.com/korobool/hlll_course/blob/master/topics/img/Step1.png?raw=1)\n",
        "\n",
        "</br>\n",
        "\n",
        "Next, we perform element-wise multiplication between the overlapping image values and filter values. Here are the results, starting from the top left corner and going right, then down:\n",
        "\n",
        "</br>\n",
        "\n",
        "![Step2](https://github.com/korobool/hlll_course/blob/master/topics/img/Step2.png?raw=1)\n",
        "\n",
        "</br>\n",
        "\n",
        "Next, we sum up all the results. That’s easy enough:\n",
        "\n",
        "$62 - 33 = 29$\n",
        " \n",
        "Finally, we place our result in the destination pixel of our output image. Since our filter is overlayed in the top left corner of the input image, our destination pixel is the top left pixel of the output image:\n",
        "\n",
        "</br>\n",
        "\n",
        "![top_left_overlayed_filter](https://github.com/korobool/hlll_course/blob/master/topics/img/top_left_overlayed_filter.png?raw=1)\n",
        "\n",
        "</br>\n",
        "\n",
        "We do the same thing to generate the rest of the output image:\n",
        "\n",
        "</br>\n",
        "\n",
        "![overlayed_filter](https://github.com/korobool/hlll_course/blob/master/topics/img/overlayed_filter.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KmQcOgJEVU62"
      },
      "source": [
        "### 3.1 How is this useful?\n",
        "\n",
        "Let’s zoom out for a second and see this at a higher level. What does convolving an image with a filter do? We can start by using the example 3x3 filter we’ve been using, which is commonly known as the vertical Sobel filter:\n",
        "\n",
        "</br>\n",
        "\n",
        "![VerticalSobelFilter](https://github.com/korobool/hlll_course/blob/master/topics/img/VerticalSobelFilter.png?raw=1)\n",
        "\n",
        "</br>\n",
        "\n",
        "Here’s an example of what the vertical Sobel filter does:\n",
        "\n",
        "</br>\n",
        "\n",
        "![An_image_convolved_vertical_Sobel_filter](https://github.com/korobool/hlll_course/blob/master/topics/img/An_image_convolved_vertical_Sobel_filter.png?raw=1)\n",
        "\n",
        "</br>\n",
        "\n",
        "Similarly, there’s also a horizontal Sobel filter:\n",
        "\n",
        "</br>\n",
        "\n",
        "![HorizontalSobelFilter](https://github.com/korobool/hlll_course/blob/master/topics/img/HorizontalSobelFilter.png?raw=1)\n",
        "\n",
        "</br>\n",
        "\n",
        "![Image_covered_horizontal_Sobel_filter](https://github.com/korobool/hlll_course/blob/master/topics/img/Image_covered_horizontal_Sobel_filter.png?raw=1)\n",
        "\n",
        "</br>\n",
        "\n",
        "See what’s happening? <b>Sobel filters are edge-detectors.</b> The vertical Sobel filter detects vertical edges, and the horizontal Sobel filter detects horizontal edges. The output images are now easily interpreted: a bright pixel (one that has a high value) in the output image indicates that there’s a strong edge around there in the original image.\n",
        "\n",
        "</br>\n",
        "\n",
        "Can you see why an edge-detected image might be more useful than the raw image? Think back to our MNIST handwritten digit classification problem for a second. A CNN trained on MNIST might look for the digit 1, for example, by using an edge-detection filter and checking for two prominent vertical edges near the center of the image. In general, <b>convolution helps us look for specific localized image features</b> (like edges) that we can use later in the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YdgARFwgXsB5"
      },
      "source": [
        "### 3.2 Padding\n",
        "\n",
        "Remember convolving a 4x4 input image with a 3x3 filter earlier to produce a 2x2 output image? Often times, we’d prefer to have the output image be the same size as the input image. To do this, we add zeros around the image so we can overlay the filter in more places. A 3x3 filter requires 1 pixel of padding:\n",
        "\n",
        "</br>\n",
        "\n",
        "![A4x4input_convolved_3x3filter_with_padding](https://github.com/korobool/hlll_course/blob/master/topics/img/A4x4input_convolved_3x3filter_with_padding.png?raw=1)\n",
        "\n",
        "</br>\n",
        "\n",
        "This is called “same” padding, since the input and output have the same dimensions. Not using any padding, which is what we’ve been doing and will continue to do for this post, is sometimes referred to as “valid” padding.\n",
        "\n",
        "### 3.3 Conv Layers\n",
        "\n",
        "Now that we know how image convolution works and why it’s useful, let’s see how it’s actually used in CNNs. As mentioned before, CNNs include <b>conv layers</b> that use a set of filters to turn input images into output images. A conv layer’s primary parameter is the <b>number of filters</b> it has.\n",
        "\n",
        "</br>\n",
        "\n",
        "For our MNIST CNN, we’ll use a small conv layer with 8 filters as the initial layer in our network. This means it’ll turn the 28x28 input image into a 26x26x8 output <b>volume</b>:\n",
        "\n",
        "</br>\n",
        "\n",
        "![conv_dimentions](https://github.com/korobool/hlll_course/blob/master/topics/img/conv_dimentions.png?raw=1)\n",
        "\n",
        "</br>\n",
        "\n",
        "<i>Reminder: The output is 26x26x8 and not 28x28x8 because we’re using valid padding, which decreases the input’s width and height by 2.</i>\n",
        "\n",
        "</br>\n",
        "\n",
        "Each of the 8 filters in the conv layer produces a 26x26 output, so stacked together they make up a 26x26x8 volume. All of this happens because of 3 \\times× 3 (filter size) \\times× 8 (number of filters) = <b>only 72 weights!</b>\n",
        "\n",
        "### 3.4 Implementing Convolution\n",
        "\n",
        "Time to put what we’ve learned into code! We’ll implement a conv layer’s feedforward portion, which takes care of convolving filters with an input image to produce an output volume. For simplicity, we’ll assume filters are always 3x3 (which is not true - 5x5 and 7x7 filters are also very common).\n",
        "\n",
        "\n",
        "</br>\n",
        "\n",
        "Let’s start implementing a conv layer class:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FzdmR98gODZs",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class Conv3x3:\n",
        "    # A Convolution layer using 3x3 filters.\n",
        "\n",
        "    def __init__(self, num_filters):\n",
        "        self.num_filters = num_filters\n",
        "\n",
        "        # filters is a 3d array with dimensions (num_filters, 3, 3)\n",
        "        # We divide by 9 to reduce the variance of our initial values\n",
        "        self.filters = np.random.randn(num_filters, 3, 3) / 9\n",
        "        \n",
        "    def iterate_regions(self, image):\n",
        "        '''\n",
        "        Generates all possible 3x3 image regions using valid padding.\n",
        "        - image is a 2d numpy array\n",
        "        '''\n",
        "        h, w = image.shape\n",
        "\n",
        "        for i in range(h - 2):\n",
        "            for j in range(w - 2):\n",
        "                im_region = image[i:(i + 3), j:(j + 3)]\n",
        "                yield im_region, i, j\n",
        "\n",
        "    def forward(self, input):\n",
        "        '''\n",
        "        Performs a forward pass of the conv layer using the given input.\n",
        "        Returns a 3d numpy array with dimensions (h, w, num_filters).\n",
        "        - input is a 2d numpy array\n",
        "        '''\n",
        "        h, w = input.shape\n",
        "        output = np.zeros((h - 2, w - 2, self.num_filters))\n",
        "\n",
        "        for im_region, i, j in self.iterate_regions(input):\n",
        "            output[i, j] = np.sum(im_region * self.filters, axis=(1, 2))\n",
        "\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xNKKKKfLZtKm"
      },
      "source": [
        "The <b>Conv3x3</b> class takes only one argument: the number of filters. In the constructor, we store the number of filters and initialize a random filters array using NumPy’s <b>randn()</b> method.\n",
        "\n",
        "</br>\n",
        "\n",
        "<i>Note: Diving by 9 during the initialization is more important than you may think. If the initial values are too large or too small, training the network will be ineffective. To learn more, read about Xavier Initialization.</i>\n",
        "\n",
        "</br>\n",
        "\n",
        "<b>iterate_regions()</b> is a helper generator method that yields all valid 3x3 image regions for us. This will be useful for implementing the backwards portion of this class later on.\n",
        "\n",
        "</br>\n",
        "\n",
        "The line of code that actually performs the convolutions is highlighted above. Let’s break it down:\n",
        "\n",
        "* We have <i>im_region</i>, a 3x3 array containing the relevant image region.\n",
        "* We have <i>self.filters</i>, a 3d array.\n",
        "* We do <i>im_region * self.filters</i>, which uses numpy’s <b>broadcasting</b> feature to element-wise multiply the two arrays. The result is a 3d array with the same dimension as <i>self.filters.</i>\n",
        "* We <i>np.sum()</i> the result of the previous step using <i>axis=(1, 2)</i>, which produces a 1d array of length <i>num_filters</i> where each element contains the convolution result for the corresponding filter.\n",
        "* We assign the result to $output[i, j]$, which contains convolution results for pixel $(i, j)$ in the output.\n",
        "\n",
        "</br>\n",
        "\n",
        "The sequence above is performed for each pixel in the output until we obtain our final output volume! Let’s give our code a test run:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8w0c5MMoPX6j",
        "outputId": "452b9ea5-0940-43fc-a9be-c170b136bce9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "!pip install mnist"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mnist in /usr/local/lib/python3.6/dist-packages (0.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from mnist) (1.16.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "004QOqc1PKp3",
        "outputId": "b87b92c5-4f5c-4705-c6f6-fb6f82f3f367",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import mnist\n",
        "\n",
        "# The mnist package handles the MNIST dataset for us!\n",
        "# Learn more at https://github.com/datapythonista/mnist\n",
        "train_images = mnist.train_images()\n",
        "train_labels = mnist.train_labels()\n",
        "\n",
        "conv = Conv3x3(8)\n",
        "output = conv.forward(train_images[0])\n",
        "print(output.shape) # (26, 26, 8)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(26, 26, 8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AdjGK371cWcy"
      },
      "source": [
        "<i>Note: in our Conv3x3 implementation, we assume the input is a <b>2d</b> numpy array for simplicity, because that’s how our MNIST images are stored. This works for us because we use it as the first layer in our network, but most CNNs have many more Conv layers. If we were building a bigger network that needed to use Conv3x3 multiple times, we’d have to make the input be a <b>3d</b> numpy array.</i>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nJ7-BldFcbnP"
      },
      "source": [
        "## 4. Pooling\n",
        "\n",
        "Neighboring pixels in images tend to have similar values, so conv layers will typically also produce similar values for neighboring pixels in outputs. As a result, <b>much of the information contained in a conv layer’s output is redundant.</b> For example, if we use an edge-detecting filter and find a strong edge at a certain location, chances are that we’ll also find relatively strong edges at locations 1 pixel shifted from the original one. However, <b>these are all the same edge!</b> We’re not finding anything new.\n",
        "\n",
        "</br>\n",
        "\n",
        "Pooling layers solve this problem. All they do is reduce the size of the input it’s given by (you guessed it) pooling values together in the input. The pooling is usually done by a simple operation like $max$, $min$, or $average$. Here’s an example of a Max Pooling layer with a pooling size of 2:\n",
        "\n",
        "</br>\n",
        "\n",
        "![MaxPooling_4x4image_2x2output](https://github.com/korobool/hlll_course/blob/master/topics/img/MaxPooling_4x4image_2x2output.png?raw=1)\n",
        "\n",
        "</br>\n",
        "\n",
        "To perform max pooling, we traverse the input image in 2x2 blocks (because pool size = 2) and put the max value into the output image at the corresponding pixel. That’s it!\n",
        "\n",
        "</br>\n",
        "\n",
        "<b>Pooling divides the input’s width and height by the pool size.</b> For our MNIST CNN, we’ll place a Max Pooling layer with a pool size of 2 right after our initial conv layer. The pooling layer will transform a 26x26x8 input into a 13x13x8 output:\n",
        "\n",
        "</br>\n",
        "\n",
        "![26x26x8input_to_13x13x8output](https://github.com/korobool/hlll_course/blob/master/topics/img/26x26x8input_to_13x13x8output.png?raw=1)\n",
        "\n",
        "</br>\n",
        "\n",
        "### 4.1 Implementing Pooling\n",
        "\n",
        "We’ll implement a $MaxPool2$ class with the same methods as our conv class from the previous section:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QRJV6J4QPQzl",
        "colab": {}
      },
      "source": [
        "class MaxPool2:\n",
        "    # A Max Pooling layer using a pool size of 2.\n",
        "\n",
        "    def iterate_regions(self, image):\n",
        "        '''\n",
        "        Generates non-overlapping 2x2 image regions to pool over.\n",
        "        - image is a 2d numpy array\n",
        "        '''\n",
        "        h, w, _ = image.shape\n",
        "        new_h = h // 2\n",
        "        new_w = w // 2\n",
        "\n",
        "        for i in range(new_h):\n",
        "            for j in range(new_w):\n",
        "                im_region = image[(i * 2):(i * 2 + 2), (j * 2):(j * 2 + 2)]\n",
        "                yield im_region, i, j\n",
        "\n",
        "    def forward(self, input):\n",
        "        '''\n",
        "        Performs a forward pass of the maxpool layer using the given input.\n",
        "        Returns a 3d numpy array with dimensions (h / 2, w / 2, num_filters).\n",
        "        - input is a 3d numpy array with dimensions (h, w, num_filters)\n",
        "        '''\n",
        "        h, w, num_filters = input.shape\n",
        "        output = np.zeros((h // 2, w // 2, num_filters))\n",
        "\n",
        "        for im_region, i, j in self.iterate_regions(input):\n",
        "            output[i, j] = np.amax(im_region, axis=(0, 1))\n",
        "\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "p5vOjfmi21Z2"
      },
      "source": [
        "This class works similarly to the <i>Conv3x3</i> class we implemented previously. The critical line is again highlighted: to find the max from a given image region, we use np.amax(), numpy’s array max method. We set <i>axis=(0, 1)</i> because we only want to maximize over the first two dimensions, height and width, and not the third, <i>num_filters</i>."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TOU6_yJds6xH",
        "outputId": "69b1a5db-8563-478e-f963-e511879b44f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# The mnist package handles the MNIST dataset for us!\n",
        "# Learn more at https://github.com/datapythonista/mnist\n",
        "train_images = mnist.train_images()\n",
        "train_labels = mnist.train_labels()\n",
        "\n",
        "conv = Conv3x3(8)\n",
        "pool = MaxPool2()\n",
        "\n",
        "output = conv.forward(train_images[0])\n",
        "output = pool.forward(output)\n",
        "print(output.shape) # (13, 13, 8)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(13, 13, 8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m3jyUbgQtKvA"
      },
      "source": [
        "## 5. Softmax\n",
        "\n",
        "To complete our CNN, we need to give it the ability to actually make predictions. We’ll do that by using the standard final layer for a multiclass classification problem: the <b>Softmax</b> layer, a fully-connected (dense) layer that uses the <b>Softmax function</b> as its activation.\n",
        "\n",
        "</br>\n",
        "\n",
        "<i>Reminder: fully-connected layers have every node connected to every output from the previous layer.\n",
        "</i>\n",
        "\n",
        "</br>\n",
        "\n",
        "If you haven’t heard of Softmax before, read my quick introduction to Softmax before continuing.\n",
        "\n",
        "### 5.1 Usage\n",
        "\n",
        "We’ll use a softmax layer with <b>10 nodes, one representing each digit</b>, as the final layer in our CNN. Each node in the layer will be connected to every input. After the softmax transformation is applied, <b>the digit represented by the node with the highest probability</b> will be the output of the CNN!\n",
        "\n",
        "</br>\n",
        "\n",
        "![28x28inpup_10softmaxoutput](https://github.com/korobool/hlll_course/blob/master/topics/img/28x28inpup_10softmaxoutput.png?raw=1)\n",
        "\n",
        "</br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ft4ghY2B5cvE"
      },
      "source": [
        "### 5.2 Cross-Entropy Loss\n",
        "\n",
        "You might have just thought to yourself, why bother transforming the outputs into probabilities? Won’t the highest output value always have the highest probability? If you did, you’re absolutely right. <b>We don’t actually need to use softmax to predict a digit </b>- we could just pick the digit with the highest output from the network!\n",
        "\n",
        "</br>\n",
        "\n",
        "What softmax really does is help us <b>quantify how sure we are of our prediction</b>, which is useful when training and evaluating our CNN. More specifically, using softmax lets us use <b>cross-entropy loss</b>, which takes into account how sure we are of each prediction. Here’s how we calculate cross-entropy loss:\n",
        "\n",
        "</br>\n",
        "\n",
        "$L = - \\ln(p_{c})$\n",
        "\n",
        "</br>\n",
        "\n",
        "where $c$ is the correct class (in our case, the correct digit), $p_{c}$\n",
        "  is the predicted probability for class $c$, and $ln$ is the natural log. As always, a lower loss is better. For example, in the best case, we’d have\n",
        "  \n",
        "  </br>\n",
        "  \n",
        "  $p_{c} = 1, L = - \\ln(1) = 0$\n",
        "  \n",
        "  </br>\n",
        "  \n",
        "  In a more realistic case, we might have\n",
        "  \n",
        "   </br>\n",
        "   \n",
        "   $p_{c} = 0.8, L = - \\ln(0.8) = 0.223$\n",
        "   \n",
        "  </br>\n",
        "    \n",
        " We’ll be seeing cross-entropy loss again later on in this post, so keep it in mind!\n",
        "\n",
        "### 5.3 Implementing Softmax\n",
        "\n",
        "Let’s implement a Softmax layer class:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ldslw6WotHBW",
        "colab": {}
      },
      "source": [
        "class Softmax:\n",
        "    # A standard fully-connected layer with softmax activation.\n",
        "\n",
        "    def __init__(self, input_len, nodes):\n",
        "        # We divide by input_len to reduce the variance of our initial values\n",
        "        self.weights = np.random.randn(input_len, nodes) / input_len\n",
        "        self.biases = np.zeros(nodes)\n",
        "\n",
        "    def forward(self, input):\n",
        "        '''\n",
        "        Performs a forward pass of the softmax layer using the given input.\n",
        "        Returns a 1d numpy array containing the respective probability values.\n",
        "        - input can be any array with any dimensions.\n",
        "        '''\n",
        "        input = input.flatten()\n",
        "\n",
        "        input_len, nodes = self.weights.shape\n",
        "\n",
        "        totals = np.dot(input, self.weights) + self.biases\n",
        "        exp = np.exp(totals)\n",
        "        return exp / np.sum(exp, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CyfZ1nrs8IXK"
      },
      "source": [
        "There’s nothing too complicated here. A few highlights:\n",
        "\n",
        "</br>\n",
        "\n",
        "* We flatten() the input to make it easier to work with, since we no longer need its shape.\n",
        "* np.dot() multiplies $input$ and $self.weights$ element-wise and then sums the results.\n",
        "* np.exp() calculates the exponentials used for Softmax.\n",
        "\n",
        "\n",
        "</br>\n",
        "\n",
        "We’ve now completed the entire forward pass of our CNN! Putting it together:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CPbM9X8ztc4M",
        "outputId": "a1016d0a-aa7c-46a1-d735-06746d227804",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        }
      },
      "source": [
        "# We only use the first 1k testing examples (out of 10k total)\n",
        "# in the interest of time. Feel free to change this if you want.\n",
        "test_images = mnist.test_images()[:1000]\n",
        "test_labels = mnist.test_labels()[:1000]\n",
        "\n",
        "conv = Conv3x3(8)                  # 28x28x1 -> 26x26x8\n",
        "pool = MaxPool2()                  # 26x26x8 -> 13x13x8\n",
        "softmax = Softmax(13 * 13 * 8, 10) # 13x13x8 -> 10\n",
        "\n",
        "def forward(image, label):\n",
        "    '''\n",
        "    Completes a forward pass of the CNN and calculates the accuracy and\n",
        "    cross-entropy loss.\n",
        "    - image is a 2d numpy array\n",
        "    - label is a digit\n",
        "    '''\n",
        "    # We transform the image from [0, 255] to [-0.5, 0.5] to make it easier\n",
        "    # to work with. This is standard practice.\n",
        "    out = conv.forward((image / 255) - 0.5)\n",
        "    out = pool.forward(out)\n",
        "    out = softmax.forward(out)\n",
        "\n",
        "    # Calculate cross-entropy loss and accuracy. np.log() is the natural log.\n",
        "    loss = -np.log(out[label])\n",
        "    acc = 1 if np.argmax(out) == label else 0\n",
        "\n",
        "    return out, loss, acc\n",
        "\n",
        "print('MNIST CNN initialized!')\n",
        "\n",
        "loss = 0\n",
        "num_correct = 0\n",
        "for i, (im, label) in enumerate(zip(test_images, test_labels)):\n",
        "    # Do a forward pass.\n",
        "    _, l, acc = forward(im, label)\n",
        "    loss += l\n",
        "    num_correct += acc\n",
        "\n",
        "    # Print stats every 100 steps.\n",
        "    if i % 100 == 99:\n",
        "        print(\n",
        "          '[Step %d] Past 100 steps: Average Loss %.3f | Accuracy: %d%%' %\n",
        "          (i + 1, loss / 100, num_correct)\n",
        "        )\n",
        "        loss = 0\n",
        "        num_correct = 0"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MNIST CNN initialized!\n",
            "[Step 100] Past 100 steps: Average Loss 2.302 | Accuracy: 8%\n",
            "[Step 200] Past 100 steps: Average Loss 2.302 | Accuracy: 11%\n",
            "[Step 300] Past 100 steps: Average Loss 2.303 | Accuracy: 8%\n",
            "[Step 400] Past 100 steps: Average Loss 2.303 | Accuracy: 7%\n",
            "[Step 500] Past 100 steps: Average Loss 2.302 | Accuracy: 9%\n",
            "[Step 600] Past 100 steps: Average Loss 2.303 | Accuracy: 4%\n",
            "[Step 700] Past 100 steps: Average Loss 2.302 | Accuracy: 8%\n",
            "[Step 800] Past 100 steps: Average Loss 2.303 | Accuracy: 10%\n",
            "[Step 900] Past 100 steps: Average Loss 2.303 | Accuracy: 5%\n",
            "[Step 1000] Past 100 steps: Average Loss 2.303 | Accuracy: 9%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4WBQVrDo-DJ0"
      },
      "source": [
        "## 6. Conclusion\n",
        "\n",
        "That’s the end of this introduction to CNNs! In this post, we\n",
        "\n",
        "</br>\n",
        "\n",
        "* Motivated why CNNs might be more useful for certain problems, like image classification.\n",
        "* Introduced the <b>MNIST</b> handwritten digit dataset.\n",
        "* Learned about <b>Conv layers</b>, which convolve filters with images to produce more useful outputs.\n",
        "* Talked about <b>Pooling layers</b>, which can help prune everything but the most useful features.\n",
        "* Implemented a <b>Softmax</b> layer so we could use <b>cross-entropy loss</b>.\n",
        "\n",
        "</br>\n",
        "\n",
        "There’s still much more that we haven’t covered yet, such as how to actually train a CNN. <b>Part 2 of this CNN series does a deep-dive on training a CNN</b>, including deriving gradients and implementing backprop.\n",
        "\n",
        "</br>\n",
        "\n",
        "If you’re eager to see a trained CNN in action: [this example Keras CNN](https://keras.io/examples/mnist_cnn/) trained on MNIST achieves <b>99.25%</b> accuracy. CNNs are powerful!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7M8fTcqdBmN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install keras tensorflow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kwtuYZRPAQmj",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "b6170806-7a74-4794-b0bd-f37e2514e505"
      },
      "source": [
        " from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 12\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKhJ7gU5dBmY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "outputId": "3497a42a-d5f6-4fb0-ebf1-928bace954fc"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0810 07:38:48.067671 140470422382464 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0810 07:38:48.087890 140470422382464 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0810 07:38:48.090363 140470422382464 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0810 07:38:48.117075 140470422382464 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0810 07:38:48.121288 140470422382464 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0810 07:38:48.130023 140470422382464 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0810 07:38:48.198463 140470422382464 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0810 07:38:48.207591 140470422382464 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 24, 24, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 9216)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               1179776   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,199,882\n",
            "Trainable params: 1,199,882\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZ-aefG2dBmb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        },
        "outputId": "74d2483c-b15e-4cd1-8cd4-2f741f3e09b0"
      },
      "source": [
        "history = model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0810 07:38:48.314105 140470422382464 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/12\n",
            "60000/60000 [==============================] - 8s 126us/step - loss: 0.2599 - acc: 0.9198 - val_loss: 0.0627 - val_acc: 0.9792\n",
            "Epoch 2/12\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0909 - acc: 0.9735 - val_loss: 0.0421 - val_acc: 0.9862\n",
            "Epoch 3/12\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0675 - acc: 0.9797 - val_loss: 0.0379 - val_acc: 0.9866\n",
            "Epoch 4/12\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0576 - acc: 0.9834 - val_loss: 0.0325 - val_acc: 0.9887\n",
            "Epoch 5/12\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0485 - acc: 0.9853 - val_loss: 0.0331 - val_acc: 0.9888\n",
            "Epoch 6/12\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0415 - acc: 0.9876 - val_loss: 0.0294 - val_acc: 0.9899\n",
            "Epoch 7/12\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0392 - acc: 0.9880 - val_loss: 0.0275 - val_acc: 0.9908\n",
            "Epoch 8/12\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0335 - acc: 0.9897 - val_loss: 0.0274 - val_acc: 0.9915\n",
            "Epoch 9/12\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0325 - acc: 0.9899 - val_loss: 0.0321 - val_acc: 0.9902\n",
            "Epoch 10/12\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0293 - acc: 0.9908 - val_loss: 0.0274 - val_acc: 0.9912\n",
            "Epoch 11/12\n",
            "60000/60000 [==============================] - 5s 75us/step - loss: 0.0264 - acc: 0.9916 - val_loss: 0.0288 - val_acc: 0.9913\n",
            "Epoch 12/12\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0260 - acc: 0.9921 - val_loss: 0.0271 - val_acc: 0.9913\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8WHR51jdBme",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "cdb818aa-7add-4687-da20-2936fd3f9566"
      },
      "source": [
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.027083164469446003\n",
            "Test accuracy: 0.9913\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Cw0r9wmemX7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yhp8X4f3dBmh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "3e2c3d74-b067-47e0-8a8e-ae88858dead6"
      },
      "source": [
        "# summarize history for accuracy\n",
        "matplotlib.pyplot.plot(history.history['acc'])\n",
        "matplotlib.pyplot.plot(history.history['val_acc'])\n",
        "matplotlib.pyplot.title('model accuracy')\n",
        "matplotlib.pyplot.ylabel('accuracy')\n",
        "matplotlib.pyplot.xlabel('epoch')\n",
        "matplotlib.pyplot.legend(['train', 'test'], loc='upper left')\n",
        "matplotlib.pyplot.show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niMiY/j9evChGNM9TKAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Seh8rGkIepS_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "1567ff51-7f6b-489a-b5cd-d8174f90c291"
      },
      "source": [
        "# summarize history for loss\n",
        "matplotlib.pyplot.plot(history.history['loss'])\n",
        "matplotlib.pyplot.plot(history.history['val_loss'])\n",
        "matplotlib.pyplot.title('model loss')\n",
        "matplotlib.pyplot.ylabel('loss')\n",
        "matplotlib.pyplot.xlabel('epoch')\n",
        "matplotlib.pyplot.legend(['train', 'test'], loc='upper left')\n",
        "matplotlib.pyplot.show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UU9jCZSXfFbZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}